<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.23.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"default","dark":"stackoverflow-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.json","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js" defer></script>

    <meta name="description" content="redis简介redis（Remote Dictionary Server）是一个开源的基于内存（也可以持久化）的日志型key-value数据库，可以用作数据库（DB）、缓存（Cache）、消息队列（MQ）等。 优势传统的数据库，例如Mysql，这类数据库的性能瓶颈越来越明显，这主要是由于磁盘IO导致的，磁盘的读写速度远不如内存读写快。 相比之下，基于内存的redis性能高于这些传统数据库。——r">
<meta property="og:type" content="article">
<meta property="og:title" content="分析scrapy-redis源码及redis学习">
<meta property="og:url" content="http://example.com/2025/07/13/%E5%88%86%E6%9E%90scrapy-redis%E6%BA%90%E7%A0%81%E5%8F%8Aredis%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="redis简介redis（Remote Dictionary Server）是一个开源的基于内存（也可以持久化）的日志型key-value数据库，可以用作数据库（DB）、缓存（Cache）、消息队列（MQ）等。 优势传统的数据库，例如Mysql，这类数据库的性能瓶颈越来越明显，这主要是由于磁盘IO导致的，磁盘的读写速度远不如内存读写快。 相比之下，基于内存的redis性能高于这些传统数据库。——r">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/X14Nuy/Picture-Bed/master/img/image-20250713091408193.png">
<meta property="og:image" content="https://raw.githubusercontent.com/X14Nuy/Picture-Bed/master/img/image-20250713150100411.png">
<meta property="og:image" content="https://raw.githubusercontent.com/X14Nuy/Picture-Bed/master/img/image-20250713154251181.png">
<meta property="article:published_time" content="2025-07-13T14:16:17.000Z">
<meta property="article:modified_time" content="2025-07-13T14:18:17.010Z">
<meta property="article:author" content="X14Nuy">
<meta property="article:tag" content="分布式爬虫&#x2F;redis&#x2F;scrapy-redis&#x2F;爬虫">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/X14Nuy/Picture-Bed/master/img/image-20250713091408193.png">


<link rel="canonical" href="http://example.com/2025/07/13/%E5%88%86%E6%9E%90scrapy-redis%E6%BA%90%E7%A0%81%E5%8F%8Aredis%E5%AD%A6%E4%B9%A0/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://example.com/2025/07/13/%E5%88%86%E6%9E%90scrapy-redis%E6%BA%90%E7%A0%81%E5%8F%8Aredis%E5%AD%A6%E4%B9%A0/","path":"2025/07/13/分析scrapy-redis源码及redis学习/","title":"分析scrapy-redis源码及redis学习"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>分析scrapy-redis源码及redis学习 | Hexo</title>
  








  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous" defer></script>
<script src="/js/third-party/search/local-search.js" defer></script>







  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Hexo</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#redis%E7%AE%80%E4%BB%8B"><span class="nav-number">1.</span> <span class="nav-text">redis简介</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%98%E5%8A%BF"><span class="nav-number">1.1.</span> <span class="nav-text">优势</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#redis%E7%9A%84%E6%8C%81%E4%B9%85%E5%8C%96"><span class="nav-number">2.</span> <span class="nav-text">redis的持久化</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%89%E8%A3%85"><span class="nav-number">3.</span> <span class="nav-text">安装</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#GUI%E5%B7%A5%E5%85%B7"><span class="nav-number">4.</span> <span class="nav-text">GUI工具</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8"><span class="nav-number">5.</span> <span class="nav-text">简单使用</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%9E%E6%8E%A5redis"><span class="nav-number">5.1.</span> <span class="nav-text">连接redis</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AD%97%E7%AC%A6%E4%B8%B2"><span class="nav-number">5.2.</span> <span class="nav-text">字符串</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%93%88%E5%B8%8C"><span class="nav-number">5.3.</span> <span class="nav-text">哈希</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%97%E8%A1%A8"><span class="nav-number">5.4.</span> <span class="nav-text">列表</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9B%86%E5%90%88"><span class="nav-number">5.5.</span> <span class="nav-text">集合</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%89%E5%BA%8F%E9%9B%86%E5%90%88"><span class="nav-number">5.6.</span> <span class="nav-text">有序集合</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#redis%E9%9B%86%E7%BE%A4"><span class="nav-number">6.</span> <span class="nav-text">redis集群</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#redis%E7%BC%93%E5%AD%98"><span class="nav-number">7.</span> <span class="nav-text">redis缓存</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#scrapy-redis%E6%BA%90%E7%A0%81%E7%90%86%E8%A7%A3"><span class="nav-number">8.</span> <span class="nav-text">scrapy-redis源码理解</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B0%83%E5%BA%A6%E5%99%A8%EF%BC%88%E6%BA%90%E6%96%87%E4%BB%B6Scheduler%EF%BC%89"><span class="nav-number">8.1.</span> <span class="nav-text">调度器（源文件Scheduler）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#init-%E6%9E%84%E9%80%A0%E5%87%BD%E6%95%B0"><span class="nav-number">8.1.1.</span> <span class="nav-text">__init__(构造函数)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#from-settings"><span class="nav-number">8.1.2.</span> <span class="nav-text">from_settings</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#classmethod%E4%BF%AE%E9%A5%B0"><span class="nav-number">8.1.2.1.</span> <span class="nav-text">@classmethod修饰</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A1%8C%E4%B8%BA"><span class="nav-number">8.1.2.2.</span> <span class="nav-text">行为</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#from-crawler"><span class="nav-number">8.1.3.</span> <span class="nav-text">from_crawler</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#open"><span class="nav-number">8.1.4.</span> <span class="nav-text">open</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#enqueue-request-next-request%EF%BC%88%E5%85%A5%E9%98%9F-%E5%87%BA%E9%98%9F%EF%BC%89"><span class="nav-number">8.1.5.</span> <span class="nav-text">enqueue_request&#x2F;next_request（入队&#x2F;出队）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8E%BB%E9%87%8D%E8%BF%87%E6%BB%A4%E5%99%A8%EF%BC%88%E6%BA%90%E6%96%87%E4%BB%B6DuperFilter%EF%BC%89"><span class="nav-number">8.2.</span> <span class="nav-text">去重过滤器（源文件DuperFilter）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#request-fingerprint"><span class="nav-number">8.2.1.</span> <span class="nav-text">request_fingerprint</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%88%AC%E8%99%AB%EF%BC%88%E6%BA%90%E6%96%87%E4%BB%B6Spiders%EF%BC%89"><span class="nav-number">8.3.</span> <span class="nav-text">爬虫（源文件Spiders）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#start-request%EF%BC%88RedisMixin%EF%BC%89"><span class="nav-number">8.3.1.</span> <span class="nav-text">start_request（RedisMixin）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#setup-redis%EF%BC%88RedisMixin%EF%BC%89"><span class="nav-number">8.3.2.</span> <span class="nav-text">setup_redis（RedisMixin）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pop-list-queue-pop-priority-queue%EF%BC%88RedisMixin%EF%BC%89"><span class="nav-number">8.3.3.</span> <span class="nav-text">pop_list_queue&#x2F;pop_priority_queue（RedisMixin）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#next-requests%EF%BC%88RedisMixin%EF%BC%89"><span class="nav-number">8.3.4.</span> <span class="nav-text">next_requests（RedisMixin）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#make-request-from-data%EF%BC%88RedisMixin%EF%BC%89"><span class="nav-number">8.3.5.</span> <span class="nav-text">make_request_from_data（RedisMixin）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#schedule-next-requests%EF%BC%88RedisMixin%EF%BC%89"><span class="nav-number">8.3.6.</span> <span class="nav-text">schedule_next_requests（RedisMixin）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#spider-idle%EF%BC%88RedisMixin%EF%BC%89"><span class="nav-number">8.3.7.</span> <span class="nav-text">spider_idle（RedisMixin）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%B1%BBRedisSpider-RedisCrawlSpider%E7%9A%84%E5%87%BD%E6%95%B0"><span class="nav-number">8.3.8.</span> <span class="nav-text">类RedisSpider&#x2F;RedisCrawlSpider的函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%A1%E9%81%93%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%EF%BC%88%E6%BA%90%E6%96%87%E4%BB%B6pipelines%EF%BC%89"><span class="nav-number">8.4.</span> <span class="nav-text">管道数据存储（源文件pipelines）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#from-crawler-1"><span class="nav-number">8.4.1.</span> <span class="nav-text">from_crawler</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#process-item"><span class="nav-number">8.4.2.</span> <span class="nav-text">process_item</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9B%B4%E5%A4%9Ascrapy%E6%89%A9%E5%B1%95%E5%8F%82%E8%80%83"><span class="nav-number">9.</span> <span class="nav-text">更多scrapy扩展参考</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">X14Nuy</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">27</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/13/%E5%88%86%E6%9E%90scrapy-redis%E6%BA%90%E7%A0%81%E5%8F%8Aredis%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="X14Nuy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="分析scrapy-redis源码及redis学习 | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          分析scrapy-redis源码及redis学习
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-07-13 22:16:17 / 修改时间：22:18:17" itemprop="dateCreated datePublished" datetime="2025-07-13T22:16:17+08:00">2025-07-13</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>32k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>59 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="redis简介"><a href="#redis简介" class="headerlink" title="redis简介"></a>redis简介</h1><p>redis（Remote Dictionary Server）是一个开源的基于<strong>内存</strong>（也可以持久化）的日志型key-value数据库，可以用作数据库（DB）、缓存（Cache）、消息队列（MQ）等。</p>
<h2 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h2><p>传统的数据库，例如Mysql，这类数据库的性能瓶颈越来越明显，这主要是由于磁盘IO导致的，磁盘的读写速度远不如内存读写快。</p>
<p>相比之下，基于内存的redis性能高于这些传统数据库。——redis是单线程（避免上下文切换）、内存操作、I&#x2F;O多路复用的集合体，故性能特别高。</p>
<h1 id="redis的持久化"><a href="#redis的持久化" class="headerlink" title="redis的持久化"></a>redis的持久化</h1><p>这篇文章讲得挺清楚：内存快照（RDB）+日志（AOF）。</p>
<p>初次连接先用快照，再通过执行增量命令，使得主从服务器达到一致状态。</p>
<p><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/2529898">https://cloud.tencent.com/developer/article/2529898</a></p>
<p>然而，一般写爬虫存储数据的时候，redis一般用于缓存和小项目的储存，这时，还需要一个NoSQL类型的数据库进行长久储存：MongoDB，之后我会学一下的。</p>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>在Linux直接apt-get安装，windows的安装比较麻烦。</p>
<h1 id="GUI工具"><a href="#GUI工具" class="headerlink" title="GUI工具"></a>GUI工具</h1><p>使用免费的开源项目。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/qishibo/AnotherRedisDesktopManager/releases/tag/v1.7.1">https://github.com/qishibo/AnotherRedisDesktopManager/releases/tag/v1.7.1</a></p>
<h1 id="简单使用"><a href="#简单使用" class="headerlink" title="简单使用"></a>简单使用</h1><p>【5分钟Python学会操作Redis数据库】 <a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1T54y1U7kb/?share_source=copy_web&vd_source=51797c11bb8b5031197f44a3ad9e668d">https://www.bilibili.com/video/BV1T54y1U7kb/?share_source=copy_web&amp;vd_source=51797c11bb8b5031197f44a3ad9e668d</a></p>
<p>下面是python的api调用。</p>
<h2 id="连接redis"><a href="#连接redis" class="headerlink" title="连接redis"></a>连接redis</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> redis</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立连接，decode_responses=True 会自动将 bytes 转为 str，选中第0个db数据库</span></span><br><span class="line">r = redis.Redis(host=<span class="string">&#x27;localhost&#x27;</span>, port=<span class="number">6379</span>, db=<span class="number">0</span>, decode_responses=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了演示，先清空当前数据库</span></span><br><span class="line">r.flushdb()</span><br></pre></td></tr></table></figure>

<h2 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. set(key, value, ex=None): 设置一个键值对，ex 是可选的过期时间（秒）</span></span><br><span class="line">r.<span class="built_in">set</span>(<span class="string">&quot;user:1:name&quot;</span>, <span class="string">&quot;Alice&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;设置 user:1:name -&gt; <span class="subst">&#123;r.get(<span class="string">&#x27;user:1:name&#x27;</span>)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. get(key): 获取一个键的值</span></span><br><span class="line">user_name = r.get(<span class="string">&quot;user:1:name&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;获取 user:1:name -&gt; <span class="subst">&#123;user_name&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. setex(key, time, value): 设置一个带过期时间的键值对</span></span><br><span class="line">r.setex(<span class="string">&quot;session:token&quot;</span>, <span class="number">60</span>, <span class="string">&quot;xyz-abc-123&quot;</span>) <span class="comment"># 60秒后过期</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;获取 session:token -&gt; <span class="subst">&#123;r.get(<span class="string">&#x27;session:token&#x27;</span>)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;session:token 的剩余时间 -&gt; <span class="subst">&#123;r.ttl(<span class="string">&#x27;session:token&#x27;</span>)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. incr(key, amount=1): 将键的值（必须是数字）增加指定的量（默认为1）</span></span><br><span class="line">r.<span class="built_in">set</span>(<span class="string">&quot;page:views&quot;</span>, <span class="number">100</span>)</span><br><span class="line">r.incr(<span class="string">&quot;page:views&quot;</span>) <span class="comment"># 增加1</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;incr page:views -&gt; <span class="subst">&#123;r.get(<span class="string">&#x27;page:views&#x27;</span>)&#125;</span>&quot;</span>)</span><br><span class="line">r.incr(<span class="string">&quot;page:views&quot;</span>, <span class="number">10</span>) <span class="comment"># 增加10</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;incr page:views by 10 -&gt; <span class="subst">&#123;r.get(<span class="string">&#x27;page:views&#x27;</span>)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. decr(key, amount=1): 将键的值减少指定的量</span></span><br><span class="line">r.decr(<span class="string">&quot;page:views&quot;</span>, <span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;decr page:views by 5 -&gt; <span class="subst">&#123;r.get(<span class="string">&#x27;page:views&#x27;</span>)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="哈希"><a href="#哈希" class="headerlink" title="哈希"></a>哈希</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. hset(key, mapping=&#123;...&#125;) 或 hset(key, field, value): 设置哈希中的字段</span></span><br><span class="line"><span class="comment"># 使用 mapping 一次性设置多个字段</span></span><br><span class="line">r.hset(<span class="string">&quot;user:1&quot;</span>, mapping=&#123;<span class="string">&quot;name&quot;</span>: <span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;age&quot;</span>: <span class="number">30</span>, <span class="string">&quot;city&quot;</span>: <span class="string">&quot;London&quot;</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 单独设置或更新一个字段</span></span><br><span class="line">r.hset(<span class="string">&quot;user:1&quot;</span>, <span class="string">&quot;email&quot;</span>, <span class="string">&quot;bob@example.com&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;设置 user:1 哈希 -&gt; <span class="subst">&#123;r.hgetall(<span class="string">&#x27;user:1&#x27;</span>)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. hget(key, field): 获取哈希中指定字段的值</span></span><br><span class="line">user_age = r.hget(<span class="string">&quot;user:1&quot;</span>, <span class="string">&quot;age&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;获取 user:1 的 age -&gt; <span class="subst">&#123;user_age&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. hgetall(key): 获取哈希中所有的字段和值，返回一个字典</span></span><br><span class="line">all_user_info = r.hgetall(<span class="string">&quot;user:1&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;获取 user:1 的所有信息 -&gt; <span class="subst">&#123;all_user_info&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. hdel(key, *fields): 删除哈希中的一个或多个字段</span></span><br><span class="line">r.hdel(<span class="string">&quot;user:1&quot;</span>, <span class="string">&quot;city&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;删除 city 字段后 -&gt; <span class="subst">&#123;r.hgetall(<span class="string">&#x27;user:1&#x27;</span>)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. hkeys(key): 获取哈希中所有的字段名</span></span><br><span class="line">fields = r.hkeys(<span class="string">&quot;user:1&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;user:1 的所有字段 -&gt; <span class="subst">&#123;fields&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. lpush(key, *values): 从列表左侧（头部）推入一个或多个值</span></span><br><span class="line">r.lpush(<span class="string">&quot;tasks&quot;</span>, <span class="string">&quot;task3&quot;</span>, <span class="string">&quot;task2&quot;</span>)</span><br><span class="line">r.lpush(<span class="string">&quot;tasks&quot;</span>, <span class="string">&quot;task4&quot;</span>) <span class="comment"># tasks 现在的顺序: [task4, task2, task3]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;lpush 后的列表 -&gt; <span class="subst">&#123;r.lrange(<span class="string">&#x27;tasks&#x27;</span>, <span class="number">0</span>, -<span class="number">1</span>)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. rpush(key, *values): 从列表右侧（尾部）推入一个或多个值</span></span><br><span class="line">r.rpush(<span class="string">&quot;tasks&quot;</span>, <span class="string">&quot;task1&quot;</span>) <span class="comment"># tasks 现在的顺序: [task4, task2, task3, task1]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;rpush 后的列表 -&gt; <span class="subst">&#123;r.lrange(<span class="string">&#x27;tasks&#x27;</span>, <span class="number">0</span>, -<span class="number">1</span>)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. lpop(key): 从列表左侧（头部）弹出一个值</span></span><br><span class="line">task = r.lpop(<span class="string">&quot;tasks&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;lpop 弹出的任务 -&gt; <span class="subst">&#123;task&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;lpop 后的列表 -&gt; <span class="subst">&#123;r.lrange(<span class="string">&#x27;tasks&#x27;</span>, <span class="number">0</span>, -<span class="number">1</span>)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. lrange(key, start, end): 获取指定范围内的元素（-1表示最后一个）</span></span><br><span class="line">all_tasks = r.lrange(<span class="string">&quot;tasks&quot;</span>, <span class="number">0</span>, -<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;lrange 获取所有任务 -&gt; <span class="subst">&#123;all_tasks&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. llen(key): 获取列表的长度</span></span><br><span class="line">num_tasks = r.llen(<span class="string">&quot;tasks&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;当前任务数量 -&gt; <span class="subst">&#123;num_tasks&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="集合"><a href="#集合" class="headerlink" title="集合"></a>集合</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. sadd(key, *values):向集合添加一个或多个成员，重复的成员会被忽略</span></span><br><span class="line">r.sadd(<span class="string">&quot;tags:post:1&quot;</span>, <span class="string">&quot;python&quot;</span>, <span class="string">&quot;redis&quot;</span>, <span class="string">&quot;database&quot;</span>)</span><br><span class="line">r.sadd(<span class="string">&quot;tags:post:1&quot;</span>, <span class="string">&quot;python&quot;</span>) <span class="comment"># 重复添加，不会有变化</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;tags:post:1 的所有标签 -&gt; <span class="subst">&#123;r.smembers(<span class="string">&#x27;tags:post:1&#x27;</span>)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. srem(key, *values): 从集合中移除一个或多个成员</span></span><br><span class="line">r.srem(<span class="string">&quot;tags:post:1&quot;</span>, <span class="string">&quot;database&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;移除 database 后的标签 -&gt; <span class="subst">&#123;r.smembers(<span class="string">&#x27;tags:post:1&#x27;</span>)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. sismember(key, value): 判断一个成员是否存在于集合中</span></span><br><span class="line">is_member = r.sismember(<span class="string">&quot;tags:post:1&quot;</span>, <span class="string">&quot;redis&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;redis 是标签吗? -&gt; <span class="subst">&#123;is_member&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. smembers(key): 获取集合中的所有成员</span></span><br><span class="line">all_tags = r.smembers(<span class="string">&quot;tags:post:1&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;smembers 获取所有标签 -&gt; <span class="subst">&#123;all_tags&#125;</span>&quot;</span>) <span class="comment"># 注意：集合是无序的</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. sunion(*keys): 返回所有给定集合的并集</span></span><br><span class="line">r.sadd(<span class="string">&quot;tags:post:2&quot;</span>, <span class="string">&quot;python&quot;</span>, <span class="string">&quot;web&quot;</span>, <span class="string">&quot;api&quot;</span>)</span><br><span class="line">union_tags = r.sunion(<span class="string">&quot;tags:post:1&quot;</span>, <span class="string">&quot;tags:post:2&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;post:1 和 post:2 标签的并集 -&gt; <span class="subst">&#123;union_tags&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="有序集合"><a href="#有序集合" class="headerlink" title="有序集合"></a>有序集合</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. zadd(key, mapping=&#123;...&#125;): 添加一个或多个成员及其分数</span></span><br><span class="line">r.zadd(<span class="string">&quot;leaderboard&quot;</span>, &#123;<span class="string">&quot;Alice&quot;</span>: <span class="number">9500</span>, <span class="string">&quot;Bob&quot;</span>: <span class="number">8700</span>, <span class="string">&quot;Cathy&quot;</span>: <span class="number">9200</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;获取排行榜所有成员 -&gt; <span class="subst">&#123;r.zrange(<span class="string">&#x27;leaderboard&#x27;</span>, <span class="number">0</span>, -<span class="number">1</span>, withscores=<span class="literal">True</span>)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. zrange(key, start, end, withscores=False): 按分数从小到大返回指定排名的成员</span></span><br><span class="line">top_3_asc = r.zrange(<span class="string">&quot;leaderboard&quot;</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;分数正序前3名 -&gt; <span class="subst">&#123;top_3_asc&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. zrevrange(key, start, end, withscores=False): 按分数从大到小返回（常用于排行榜）</span></span><br><span class="line">top_3_desc = r.zrevrange(<span class="string">&quot;leaderboard&quot;</span>, <span class="number">0</span>, <span class="number">2</span>, withscores=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;分数倒序前3名（排行榜）-&gt; <span class="subst">&#123;top_3_desc&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. zrem(key, *values): 移除一个或多个成员</span></span><br><span class="line">r.zrem(<span class="string">&quot;leaderboard&quot;</span>, <span class="string">&quot;Bob&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;移除 Bob 后的排行榜 -&gt; <span class="subst">&#123;r.zrevrange(<span class="string">&#x27;leaderboard&#x27;</span>, <span class="number">0</span>, -<span class="number">1</span>, withscores=<span class="literal">True</span>)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. zscore(key, member): 获取指定成员的分数</span></span><br><span class="line">cathy_score = r.zscore(<span class="string">&quot;leaderboard&quot;</span>, <span class="string">&quot;Cathy&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Cathy 的分数 -&gt; <span class="subst">&#123;cathy_score&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h1 id="redis集群"><a href="#redis集群" class="headerlink" title="redis集群"></a>redis集群</h1><p>数据分片、主从复制。</p>
<p>redis集群意味着数据需要分片，服务器会根据数据的key，生成哈希值，哈希值不同，该数据存放的哈希槽就不同，不同的主节点负责不同的哈希槽。</p>
<p>每个主节点都有一个或多个从节点，为保证数据的可用性，还需保证主从复制。</p>
<img src="https://raw.githubusercontent.com/X14Nuy/Picture-Bed/master/img/image-20250713091408193.png" alt="image-20250713091408193" style="zoom:80%;" />

<h1 id="redis缓存"><a href="#redis缓存" class="headerlink" title="redis缓存"></a>redis缓存</h1><p>由于redis是内存操作，不用从硬盘中取数据，常被用作缓存，一般的<strong>读取数据</strong>流程如下（<strong>旁路缓存，Cache-Aside</strong>）：</p>
<ol>
<li><p>应用先请求 <strong>Redis 缓存</strong>，看有没有需要的数据。</p>
</li>
<li><p><strong>缓存命中 (Cache Hit)</strong>: 如果 Redis 中有数据，则直接读取并返回给用户。流程结束。</p>
</li>
<li><p><strong>缓存未命中 (Cache Miss)</strong>: 如果 Redis 中没有数据，则去访问<strong>后端数据库</strong>。</p>
</li>
<li><p>从数据库中查到数据后，<strong>先将其写入 Redis 缓存</strong>（并设置一个合适的过期时间），然后再返回给用户。这样，下次再有同样的请求，就能直接命中缓存了。</p>
</li>
</ol>
<p><strong>更新数据 (Write)</strong>:</p>
<p>当数据发生变化时（例如用户修改了个人信息），为了保证数据一致性，必须同时处理数据库和缓存。常见的做法是：</p>
<ol>
<li>先更新<strong>数据库</strong>中的数据。</li>
<li>然后直接<strong>从缓存中删除</strong>这个 key。</li>
</ol>
<p><strong>为什么是删除缓存而不是更新缓存？</strong> 这是一种叫做“懒加载”（Lazy Loading）的思想。直接删除可以保证下次读取时一定会从数据库加载最新的数据并重新写入缓存。这比直接更新缓存要简单，并且可以避免一些并发场景下的数据不一致问题。</p>
<h1 id="scrapy-redis源码理解"><a href="#scrapy-redis源码理解" class="headerlink" title="scrapy-redis源码理解"></a>scrapy-redis源码理解</h1><p>第三方库scrapy-redis已经封装好了scrapy和redis的联合使用，实现分布式爬虫。——实际上，分布式爬虫的含义是：几个爬虫一起爬取数据，共享<strong>任务列表、调度器、数据存储表</strong>等，还具备<strong>断点续爬</strong>的特点（因为有redis数据库）。</p>
<p>来看看scrapy-redis是如何接入redis的，先看看源文件，相比之前的项目多了“connection.py”、“defaults.py”、“picklecompat.py”、“duperfilter.py”、“queue.py”、“stats.py”、“utils.py”，同时还修改了pipelines.py等文件。</p>
<p>接下来会按照<strong>调度器、去重过滤器、爬虫、管道</strong>的顺序详细介绍。</p>
<img src="https://raw.githubusercontent.com/X14Nuy/Picture-Bed/master/img/image-20250713150100411.png" alt="image-20250713150100411" style="zoom:80%;" />

<h2 id="调度器（源文件Scheduler）"><a href="#调度器（源文件Scheduler）" class="headerlink" title="调度器（源文件Scheduler）"></a>调度器（源文件Scheduler）</h2><p>调度器的默认类是<code> scrapy.core.scheduler.Scheduler</code>，可以在<code>settings.py</code>中，通过<code>SCHEDULER</code>修改，官方文档如下：</p>
<p><img src="https://raw.githubusercontent.com/X14Nuy/Picture-Bed/master/img/image-20250713154251181.png" alt="image-20250713154251181"></p>
<p>如果要使用scrapy-redis，需要将<code>SCHEDULER</code>设置成<code>“&lt;模块&gt;.Scheduler”</code>。</p>
<h3 id="init-构造函数"><a href="#init-构造函数" class="headerlink" title="__init__(构造函数)"></a><code>__init__</code>(构造函数)</h3><p>类Scheduler构造函数，用来实例化调度器。</p>
<p>注释已经把构造函数的字段介绍的很清楚了，这里翻译成中文好了。</p>
<ul>
<li><strong><code>server</code></strong>: 这是与 Redis 数据库建立的连接实例。所有后续的数据库操作（如读写队列）都通过这个对象来执行。</li>
</ul>
<ul>
<li><strong><code>persist</code></strong>: 一个布尔值，用于决定爬虫关闭时是否“持久化”队列。如果设为 <code>True</code>，请求队列和去重记录会保留在 Redis 中，方便下次启动时继续爬取。如果为 <code>False</code> (默认值)，爬虫关闭时会清空这些数据。</li>
<li><strong><code>flush_on_start</code></strong>: 一个布尔值，用于决定爬虫启动时是否清空之前的队列。如果设为 <code>True</code>，每次启动都会忽略上次遗留的请求，进行一次全新的爬取。</li>
<li><strong><code>queue_key</code></strong>: 字符串类型，定义了在 Redis 中存储请求队列的键（key）的格式。默认值是 <code>%(spider)s:requests</code>，这意味着每个爬虫都有自己独立的请求队列。</li>
<li><strong><code>queue_cls</code></strong>: 字符串类型，指向一个实现了队列功能的类的完整路径。这允许你更换不同的排队策略，例如先进先出（FIFO）、后进先出（LIFO）或默认的优先级队列（PriorityQueue）。</li>
<li><strong><code>dupefilter</code></strong>: 这是一个去重过滤器实例。通常情况下，调度器会根据 <code>dupefilter_cls</code> 自己创建一个实例。但这个字段允许你传入一个已经手动配置好的实例，提供了更高的灵活性。</li>
<li><strong><code>dupefilter_key</code></strong>: 字符串类型，定义了在 Redis 中存储请求指纹（用于去重）的键的格式。默认值是<code>%(spider)s:dupefilter</code>。</li>
<li><strong><code>dupefilter_cls</code></strong>: 字符串类型，指向一个实现了去重逻辑的类的完整路径。默认是<code>scrapy_redis.dupefilter.RFPDupeFilter</code>。</li>
<li><strong><code>idle_before_close</code></strong>: 整数类型，表示在队列变空后，调度器在关闭爬虫前应等待多少秒。如果在这期间有新的请求加入队列，爬虫会继续工作。这对于等待动态添加任务的持续性爬虫非常重要。</li>
<li><strong><code>serializer</code></strong>: 这是一个序列化器对象。它负责将 Scrapy 的请求对象（Request）转换成可以存入 Redis 的格式（通常是字符串或字节），以及在从 Redis 取出时进行反向转换。</li>
<li><strong><code>stats</code></strong>: 用于存储一个统计信息收集器实例，它会在调度器工作时记录相关数据（如入队&#x2F;出队的请求数）。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">    self,</span></span><br><span class="line"><span class="params">    server,</span></span><br><span class="line"><span class="params">    persist=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    flush_on_start=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    queue_key=defaults.SCHEDULER_QUEUE_KEY,</span></span><br><span class="line"><span class="params">    queue_cls=defaults.SCHEDULER_QUEUE_CLASS,</span></span><br><span class="line"><span class="params">    dupefilter=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    dupefilter_key=defaults.SCHEDULER_DUPEFILTER_KEY,</span></span><br><span class="line"><span class="params">    dupefilter_cls=defaults.SCHEDULER_DUPEFILTER_CLASS,</span></span><br><span class="line"><span class="params">    idle_before_close=<span class="number">0</span>,</span></span><br><span class="line"><span class="params">    serializer=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Initialize scheduler.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    server : Redis</span></span><br><span class="line"><span class="string">        The redis server instance.</span></span><br><span class="line"><span class="string">    persist : bool</span></span><br><span class="line"><span class="string">        Whether to flush requests when closing. Default is False.</span></span><br><span class="line"><span class="string">    flush_on_start : bool</span></span><br><span class="line"><span class="string">        Whether to flush requests on start. Default is False.</span></span><br><span class="line"><span class="string">    queue_key : str</span></span><br><span class="line"><span class="string">        Requests queue key.</span></span><br><span class="line"><span class="string">    queue_cls : str</span></span><br><span class="line"><span class="string">        Importable path to the queue class.</span></span><br><span class="line"><span class="string">    dupefilter: Dupefilter</span></span><br><span class="line"><span class="string">        Custom dupefilter instance.</span></span><br><span class="line"><span class="string">    dupefilter_key : str</span></span><br><span class="line"><span class="string">        Duplicates filter key.</span></span><br><span class="line"><span class="string">    dupefilter_cls : str</span></span><br><span class="line"><span class="string">        Importable path to the dupefilter class.</span></span><br><span class="line"><span class="string">    idle_before_close : int</span></span><br><span class="line"><span class="string">        Timeout before giving up.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> idle_before_close &lt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">raise</span> TypeError(<span class="string">&quot;idle_before_close cannot be negative&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="variable language_">self</span>.server = server</span><br><span class="line">    <span class="variable language_">self</span>.persist = persist</span><br><span class="line">    <span class="variable language_">self</span>.flush_on_start = flush_on_start</span><br><span class="line">    <span class="variable language_">self</span>.queue_key = queue_key</span><br><span class="line">    <span class="variable language_">self</span>.queue_cls = queue_cls</span><br><span class="line">    <span class="variable language_">self</span>.df = dupefilter</span><br><span class="line">    <span class="variable language_">self</span>.dupefilter_cls = dupefilter_cls</span><br><span class="line">    <span class="variable language_">self</span>.dupefilter_key = dupefilter_key</span><br><span class="line">    <span class="variable language_">self</span>.idle_before_close = idle_before_close</span><br><span class="line">    <span class="variable language_">self</span>.serializer = serializer</span><br><span class="line">    <span class="variable language_">self</span>.stats = <span class="literal">None</span></span><br></pre></td></tr></table></figure>

<h3 id="from-settings"><a href="#from-settings" class="headerlink" title="from_settings"></a>from_settings</h3><h4 id="classmethod修饰"><a href="#classmethod修饰" class="headerlink" title="@classmethod修饰"></a><code>@classmethod</code>修饰</h4><p>这个函数使用了<code>@classmethod</code>修饰，是说明这个方法是一个工厂方法，它的作用是<strong>创建并返回一个类的实例</strong>，而不是操作一个已存在的实例，<code>@classmethod</code>修饰的方法，其第一个参数需要是类本身，而不是实例self。</p>
<h4 id="行为"><a href="#行为" class="headerlink" title="行为"></a>行为</h4><p><strong>加载核心设置</strong>: 它首先从 <code>settings</code> 中读取 <code>SCHEDULER_PERSIST</code>、<code>SCHEDULER_FLUSH_ON_START</code> 和 <code>SCHEDULER_IDLE_BEFORE_CLOSE</code> 设置，并将它们存入一个名为 <code>kwargs</code> 的字典中 。</p>
<p><strong>加载可选设置</strong>: 它会遍历一个预定义的 <code>optional</code> 字典，检查 <code>settings</code> 中是否存在如 <code>SCHEDULER_QUEUE_CLASS</code> 或 <code>DUPEFILTER_CLASS</code> 等可选配置 。如果存在，就将它们也添加到 <code>kwargs</code> 字典中 。这使得用户可以只配置需要覆盖的选项，而其它选项则使用默认值。</p>
<p><strong>处理去重过滤器</strong>: 它使用 <code>load_object</code> 函数加载去重过滤器类 。接着，它会检查这个类是否需要基于 <code>spider</code> 对象进行初始化（通过检查有无 <code>from_spider</code> 方法）。如果不需要，它会立刻用 <code>from_settings</code> 方法创建一个实例，并存入 <code>kwargs</code> 。</p>
<p><strong>处理序列化器</strong>: 如果 <code>serializer</code> 是以字符串路径的形式提供的，它会使用 <code>importlib.import_module</code> 将其作为一个模块加载进来 。</p>
<p><strong>建立并验证 Redis 连接</strong>: 它调用 <code>connection.from_settings</code> 来创建一个 Redis 连接实例，然后立刻执行 <code>server.ping()</code> 以确保该连接是真实有效的 。</p>
<p><strong>创建并返回调度器实例</strong>: 最后，它使用 <code>cls(server=server, **kwargs)</code>，将 Redis 连接和所有从配置中收集到的参数传递给 <code>Scheduler</code> 的构造函数，创建一个完全配置好的实例并返回。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_settings</span>(<span class="params">cls, settings</span>):</span><br><span class="line">        kwargs = &#123;</span><br><span class="line">            <span class="string">&quot;persist&quot;</span>: settings.getbool(<span class="string">&quot;SCHEDULER_PERSIST&quot;</span>),</span><br><span class="line">            <span class="string">&quot;flush_on_start&quot;</span>: settings.getbool(<span class="string">&quot;SCHEDULER_FLUSH_ON_START&quot;</span>),</span><br><span class="line">            <span class="string">&quot;idle_before_close&quot;</span>: settings.getint(<span class="string">&quot;SCHEDULER_IDLE_BEFORE_CLOSE&quot;</span>),</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># If these values are missing, it means we want to use the defaults.</span></span><br><span class="line">        optional = &#123;</span><br><span class="line">            <span class="comment"># <span class="doctag">TODO:</span> Use custom prefixes for this settings to note that are</span></span><br><span class="line">            <span class="comment"># specific to scrapy-redis.</span></span><br><span class="line">            <span class="string">&quot;queue_key&quot;</span>: <span class="string">&quot;SCHEDULER_QUEUE_KEY&quot;</span>,</span><br><span class="line">            <span class="string">&quot;queue_cls&quot;</span>: <span class="string">&quot;SCHEDULER_QUEUE_CLASS&quot;</span>,</span><br><span class="line">            <span class="string">&quot;dupefilter_key&quot;</span>: <span class="string">&quot;SCHEDULER_DUPEFILTER_KEY&quot;</span>,</span><br><span class="line">            <span class="comment"># We use the default setting name to keep compatibility.</span></span><br><span class="line">            <span class="string">&quot;dupefilter_cls&quot;</span>: <span class="string">&quot;DUPEFILTER_CLASS&quot;</span>,</span><br><span class="line">            <span class="string">&quot;serializer&quot;</span>: <span class="string">&quot;SCHEDULER_SERIALIZER&quot;</span>,</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> name, setting_name <span class="keyword">in</span> optional.items():</span><br><span class="line">            val = settings.get(setting_name)</span><br><span class="line">            <span class="keyword">if</span> val:</span><br><span class="line">                kwargs[name] = val</span><br><span class="line"></span><br><span class="line">        dupefilter_cls = load_object(kwargs[<span class="string">&quot;dupefilter_cls&quot;</span>])</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(dupefilter_cls, <span class="string">&quot;from_spider&quot;</span>):</span><br><span class="line">            kwargs[<span class="string">&quot;dupefilter&quot;</span>] = dupefilter_cls.from_settings(settings)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Support serializer as a path to a module.</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(kwargs.get(<span class="string">&quot;serializer&quot;</span>), <span class="built_in">str</span>):</span><br><span class="line">            kwargs[<span class="string">&quot;serializer&quot;</span>] = importlib.import_module(kwargs[<span class="string">&quot;serializer&quot;</span>])</span><br><span class="line"></span><br><span class="line">        server = connection.from_settings(settings)</span><br><span class="line">        <span class="comment"># Ensure the connection is working.</span></span><br><span class="line">        server.ping()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> cls(server=server, **kwargs)</span><br></pre></td></tr></table></figure>

<h3 id="from-crawler"><a href="#from-crawler" class="headerlink" title="from_crawler"></a>from_crawler</h3><p>对象Crawler是Scrapy框架的核心，它持有对所有关键组件和设置引用，这里便是通过from_crawler的设置信息，初始化一个调度器实例。</p>
<p>这里的cls是<code>SCHEDULER</code>中指向的类，也就是这里的Scheduler。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@classmethod</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">from_crawler</span>(<span class="params">cls, crawler</span>):</span><br><span class="line">    instance = cls.from_settings(crawler.settings)</span><br><span class="line">    <span class="comment"># <span class="doctag">FIXME:</span> for now, stats are only supported from this constructor</span></span><br><span class="line">    instance.stats = crawler.stats</span><br><span class="line">    <span class="keyword">return</span> instance</span><br></pre></td></tr></table></figure>

<p>而from_crawler也是Scrapy框架中，调度器最小接口的函数之一，由框架代码调用。</p>
<h3 id="open"><a href="#open" class="headerlink" title="open"></a>open</h3><p>函数open用于获得<strong>请求队列</strong>和<strong>去重过滤器</strong>的key，之后通过其它函数获得这些key的具体值。</p>
<p>哦，对了，函数open也是调度器最小接口的函数之一。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">open</span>(<span class="params">self, spider</span>):</span><br><span class="line">        <span class="variable language_">self</span>.spider = spider</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="variable language_">self</span>.queue = load_object(<span class="variable language_">self</span>.queue_cls)(</span><br><span class="line">                server=<span class="variable language_">self</span>.server,</span><br><span class="line">                spider=spider,</span><br><span class="line">                key=<span class="variable language_">self</span>.queue_key % &#123;<span class="string">&quot;spider&quot;</span>: spider.name&#125;,</span><br><span class="line">                serializer=<span class="variable language_">self</span>.serializer,</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">except</span> TypeError <span class="keyword">as</span> e:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">f&quot;Failed to instantiate queue class &#x27;<span class="subst">&#123;self.queue_cls&#125;</span>&#x27;: <span class="subst">&#123;e&#125;</span>&quot;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.df:</span><br><span class="line">            <span class="variable language_">self</span>.df = load_object(<span class="variable language_">self</span>.dupefilter_cls).from_spider(spider)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.flush_on_start:</span><br><span class="line">            <span class="variable language_">self</span>.flush()</span><br><span class="line">        <span class="comment"># notice if there are requests already in the queue to resume the crawl</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.queue):</span><br><span class="line">            spider.log(<span class="string">f&quot;Resuming crawl (<span class="subst">&#123;<span class="built_in">len</span>(self.queue)&#125;</span> requests scheduled)&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><code>self.queue</code>通过类<code>self.queue_cls</code>和函数<code>load_object</code>获得一个队列实例，类<code>self.queue_cls</code>实际上就是字符串<code>SCHEDULER_QUEUE_CLASS = &quot;scrapy_redis.queue.PriorityQueue&quot;</code>，函数<code>load_object</code>通过类的路径，对类进行加载，然后以<code>类()</code>的方式调用构造函数进行实例化。<code>self.queue</code>指向字符串<code>SCHEDULER_QUEUE_KEY = &quot;%(spider)s:requests&quot;</code>，这个队列对象内部保存了要操作的 Redis 键名 (key)，这个键名才是由 <code>SCHEDULER_QUEUE_KEY</code> 格式化而来的字符串。key为”{spider.name}:requests”的value，如果爬虫名字设置为”x14nuy”，这里的key的值就是”x14nuy:requests”。</p>
<p>（然后取出这个键中存储的<code>requests</code>请求对象，如何取出呢？通过<code>self.serializer</code>进行反序列化）</p>
<p>当然，构造函数只是设置了<strong>请求队列</strong>和<strong>去重过滤器</strong>的key，还没从redis中取数据，我这里为了方便描述，把取的过程也算入了构造函数中，这是不对的。</p>
<p>真正取值的函数是enqueue_request&#x2F;next_request。</p>
<h3 id="enqueue-request-next-request（入队-出队）"><a href="#enqueue-request-next-request（入队-出队）" class="headerlink" title="enqueue_request&#x2F;next_request（入队&#x2F;出队）"></a>enqueue_request&#x2F;next_request（入队&#x2F;出队）</h3><p>enqueue是入队，next是出队。</p>
<p><code>request.dont_filter</code>是判断当前request请求，是否被特殊标记为“不过滤”，<code>df</code>即DupeFilter，用来判断当前的request是否出现过。</p>
<p>这里的self.stats是用来统计数据的，指向的是类<code>RedisStatsCollector</code>的实例，用来收集信息。</p>
<p><code>self.queue.push(request)</code>入队一个请求。</p>
<p><code>request = self.queue.pop(block_pop_timeout)</code>出队一个请求，<code>block_pop_timeout</code>用于当队列为空时，阻塞一段时机再返回——这段阻塞的时候，可能会获得新请求，如果没用获得新请求，爬虫进程就结束了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">enqueue_request</span>(<span class="params">self, request</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> request.dont_filter <span class="keyword">and</span> <span class="variable language_">self</span>.df.request_seen(request):</span><br><span class="line">        <span class="variable language_">self</span>.df.log(request, <span class="variable language_">self</span>.spider)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.stats:</span><br><span class="line">        <span class="variable language_">self</span>.stats.inc_value(<span class="string">&quot;scheduler/enqueued/redis&quot;</span>, spider=<span class="variable language_">self</span>.spider)</span><br><span class="line">    <span class="variable language_">self</span>.queue.push(request)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">next_request</span>(<span class="params">self</span>):</span><br><span class="line">    block_pop_timeout = <span class="variable language_">self</span>.idle_before_close</span><br><span class="line">    request = <span class="variable language_">self</span>.queue.pop(block_pop_timeout)</span><br><span class="line">    <span class="keyword">if</span> request <span class="keyword">and</span> <span class="variable language_">self</span>.stats:</span><br><span class="line">        <span class="variable language_">self</span>.stats.inc_value(<span class="string">&quot;scheduler/dequeued/redis&quot;</span>, spider=<span class="variable language_">self</span>.spider)</span><br><span class="line">    <span class="keyword">return</span> request</span><br></pre></td></tr></table></figure>

<h2 id="去重过滤器（源文件DuperFilter）"><a href="#去重过滤器（源文件DuperFilter）" class="headerlink" title="去重过滤器（源文件DuperFilter）"></a>去重过滤器（源文件DuperFilter）</h2><p>在阅读调度器的代码的时候，出现了<code>self.df.request_seen(request)</code>，这里的df是类DuperFilter的实例。</p>
<p>这里主要看看如何去重的。</p>
<p>锁定函数request_fingerprint。</p>
<h3 id="request-fingerprint"><a href="#request-fingerprint" class="headerlink" title="request_fingerprint"></a>request_fingerprint</h3><p>将请求的method、url、body构成字典，然后转换成json文本，通过sha1算法得到哈希值。</p>
<p>from_crawler：用于创建一次性的redis键，用来记录requests产生的哈希值，下次这些哈希值就用不上了。——因为key使用的是时间函数。</p>
<p>而from_spider：用于持续性的收集requests的哈希值，即便这次进程退出了，redis存储的哈希值在程序下一次跑起来时，仍然存在并筛选重复的requests。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">request_fingerprint</span>(<span class="params">self, request</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Returns a fingerprint for a given request.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    request : scrapy.http.Request</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    str</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    fingerprint_data = &#123;</span><br><span class="line">        <span class="string">&quot;method&quot;</span>: to_unicode(request.method),</span><br><span class="line">        <span class="string">&quot;url&quot;</span>: canonicalize_url(request.url),</span><br><span class="line">        <span class="string">&quot;body&quot;</span>: (request.body <span class="keyword">or</span> <span class="string">b&quot;&quot;</span>).<span class="built_in">hex</span>(),</span><br><span class="line">    &#125;</span><br><span class="line">    fingerprint_json = json.dumps(fingerprint_data, sort_keys=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> hashlib.sha1(fingerprint_json.encode()).hexdigest()</span><br></pre></td></tr></table></figure>



<h2 id="爬虫（源文件Spiders）"><a href="#爬虫（源文件Spiders）" class="headerlink" title="爬虫（源文件Spiders）"></a>爬虫（源文件Spiders）</h2><p>在文件spiders.py中，定义了3个类：RedisMixin、RedisSpider、RedisCrawlSpider。</p>
<p>在原来的Scrapy框架中，也存在2个类，<code>Spider</code>和<code>CrawlSpider</code>，后者比前者多了基于 <code>Rule</code> 的链接提取和自动跟进能力。</p>
<p><code>RedisMixin</code>是一个混合类，相当于一个集成了redis的插件，任何类继承了它，就拥有了从redis读取url的能力。</p>
<h3 id="start-request（RedisMixin）"><a href="#start-request（RedisMixin）" class="headerlink" title="start_request（RedisMixin）"></a>start_request（RedisMixin）</h3><p>这个函数是Scrapy框架中，爬虫启动调用的第一个方法，直接封装了self.next_requests()来获得request。</p>
<h3 id="setup-redis（RedisMixin）"><a href="#setup-redis（RedisMixin）" class="headerlink" title="setup_redis（RedisMixin）"></a>setup_redis（RedisMixin）</h3><p>这个函数负责建立与redis的连接，其它的组件可以通过self.server获得redis的数据。</p>
<p>**获得配置：**从<code>settings.py</code>中读取与Redis相关的配置，如<code>redis_key</code>（URL存储在Redis中的键名）、<code>redis_batch_size</code>（每次从Redis中取出的URL数量）、<code>redis_encoding</code>（编码格式）等。如果用户没有在自己的爬虫里定义这些属性，就使用配置文件中的默认值。——<code>crawler.settings</code>对应着文件<code>settings.py</code>。</p>
<p>**建立连接：**它使用<code>connection.from_settings()</code>根据Scrapy项目中的<code>settings.py</code>配置来建立与Redis服务器的连接——redis的类是：<code>REDIS_CLS = redis.StrictRedis</code>，先加载类，然后通过from_url或者构造函数建立连接。</p>
<p><strong>确定数据类型：<strong>根据配置判断URL在Redis中是作为</strong>列表(list)</strong>、**集合(set)<strong>还是</strong>有序集合(zset)**存储的。并据此将<code>self.fetch_data</code>（获取数据的方法）指向对应的弹出操作函数（如<code>pop_list_queue</code>）。这使得爬虫可以灵活地处理不同类型的Redis数据结构。</p>
<p>**处理信号：**当没有待处理的请求时，会触发信号<code>spider_idle</code>，将这个信号和<code>self.spider_idle</code>方法关联起来，避免直接关闭爬虫。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">setup_redis</span>(<span class="params">self, crawler=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Setup redis connection and idle signal.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This should be called after the spider has set its crawler object.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.server <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> crawler <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># We allow optional crawler argument to keep backwards</span></span><br><span class="line">        <span class="comment"># compatibility.</span></span><br><span class="line">        <span class="comment"># <span class="doctag">XXX:</span> Raise a deprecation warning.</span></span><br><span class="line">        crawler = <span class="built_in">getattr</span>(<span class="variable language_">self</span>, <span class="string">&quot;crawler&quot;</span>, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> crawler <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;crawler is required&quot;</span>)</span><br><span class="line"></span><br><span class="line">    settings = crawler.settings</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.redis_key <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="variable language_">self</span>.redis_key = settings.get(</span><br><span class="line">            <span class="string">&quot;REDIS_START_URLS_KEY&quot;</span>,</span><br><span class="line">            defaults.START_URLS_KEY,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="variable language_">self</span>.redis_key = <span class="variable language_">self</span>.redis_key % &#123;<span class="string">&quot;name&quot;</span>: <span class="variable language_">self</span>.name&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.redis_key.strip():</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;redis_key must not be empty&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.redis_batch_size <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="variable language_">self</span>.redis_batch_size = settings.getint(</span><br><span class="line">            <span class="string">&quot;CONCURRENT_REQUESTS&quot;</span>, defaults.REDIS_CONCURRENT_REQUESTS</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="variable language_">self</span>.redis_batch_size = <span class="built_in">int</span>(<span class="variable language_">self</span>.redis_batch_size)</span><br><span class="line">    <span class="keyword">except</span> (TypeError, ValueError):</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;redis_batch_size must be an integer&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.redis_encoding <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="variable language_">self</span>.redis_encoding = settings.get(</span><br><span class="line">            <span class="string">&quot;REDIS_ENCODING&quot;</span>, defaults.REDIS_ENCODING</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="variable language_">self</span>.logger.info(</span><br><span class="line">        <span class="string">&quot;Reading start URLs from redis key &#x27;%(redis_key)s&#x27; &quot;</span></span><br><span class="line">        <span class="string">&quot;(batch size: %(redis_batch_size)s, encoding: %(redis_encoding)s)&quot;</span>,</span><br><span class="line">        <span class="variable language_">self</span>.__dict__,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="variable language_">self</span>.server = connection.from_settings(crawler.settings)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> settings.getbool(<span class="string">&quot;REDIS_START_URLS_AS_SET&quot;</span>, defaults.START_URLS_AS_SET):</span><br><span class="line">        <span class="variable language_">self</span>.fetch_data = <span class="variable language_">self</span>.server.spop</span><br><span class="line">        <span class="variable language_">self</span>.count_size = <span class="variable language_">self</span>.server.scard</span><br><span class="line">    <span class="keyword">elif</span> settings.getbool(<span class="string">&quot;REDIS_START_URLS_AS_ZSET&quot;</span>, defaults.START_URLS_AS_ZSET):</span><br><span class="line">        <span class="variable language_">self</span>.fetch_data = <span class="variable language_">self</span>.pop_priority_queue</span><br><span class="line">        <span class="variable language_">self</span>.count_size = <span class="variable language_">self</span>.server.zcard</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="variable language_">self</span>.fetch_data = <span class="variable language_">self</span>.pop_list_queue</span><br><span class="line">        <span class="variable language_">self</span>.count_size = <span class="variable language_">self</span>.server.llen</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.max_idle_time <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="variable language_">self</span>.max_idle_time = settings.get(</span><br><span class="line">            <span class="string">&quot;MAX_IDLE_TIME_BEFORE_CLOSE&quot;</span>, defaults.MAX_IDLE_TIME</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="variable language_">self</span>.max_idle_time = <span class="built_in">int</span>(<span class="variable language_">self</span>.max_idle_time)</span><br><span class="line">    <span class="keyword">except</span> (TypeError, ValueError):</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;max_idle_time must be an integer&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># The idle signal is called when the spider has no requests left,</span></span><br><span class="line">    <span class="comment"># that&#x27;s when we will schedule new requests from redis queue</span></span><br><span class="line">    crawler.signals.connect(<span class="variable language_">self</span>.spider_idle, signal=signals.spider_idle)</span><br></pre></td></tr></table></figure>

<h3 id="pop-list-queue-pop-priority-queue（RedisMixin）"><a href="#pop-list-queue-pop-priority-queue（RedisMixin）" class="headerlink" title="pop_list_queue&#x2F;pop_priority_queue（RedisMixin）"></a>pop_list_queue&#x2F;pop_priority_queue（RedisMixin）</h3><p>获取redis的request数据。</p>
<p><strong><code>pop_list_queue</code></strong>: 如果URL存储在**列表(list)**中，此方法会被调用。它原子性地（通过<code>pipeline</code>）获取列表头部的<code>batch_size</code>个URL，并从列表中删除它们。</p>
<p><strong><code>pop_priority_queue</code></strong>: 如果URL存储在**有序集合(zset)**中（用于优先级队列），此方法会被调用。它获取分数最高的<code>batch_size</code>个URL，并从集合中删除它们。</p>
<p>这两个函数在setup_redis中，会传地址给函数fetch_data。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">pop_list_queue</span>(<span class="params">self, redis_key, batch_size</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="variable language_">self</span>.server.pipeline() <span class="keyword">as</span> pipe:</span><br><span class="line">        pipe.lrange(redis_key, <span class="number">0</span>, batch_size - <span class="number">1</span>)</span><br><span class="line">        pipe.ltrim(redis_key, batch_size, -<span class="number">1</span>)</span><br><span class="line">        datas, _ = pipe.execute()</span><br><span class="line">    <span class="keyword">return</span> datas</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pop_priority_queue</span>(<span class="params">self, redis_key, batch_size</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="variable language_">self</span>.server.pipeline() <span class="keyword">as</span> pipe:</span><br><span class="line">        pipe.zrevrange(redis_key, <span class="number">0</span>, batch_size - <span class="number">1</span>)</span><br><span class="line">        pipe.zremrangebyrank(redis_key, -batch_size, -<span class="number">1</span>)</span><br><span class="line">        datas, _ = pipe.execute()</span><br><span class="line">    <span class="keyword">return</span> datas</span><br></pre></td></tr></table></figure>

<h3 id="next-requests（RedisMixin）"><a href="#next-requests（RedisMixin）" class="headerlink" title="next_requests（RedisMixin）"></a>next_requests（RedisMixin）</h3><p>调用fecth_data获取数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">next_requests</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Returns a request to be scheduled or none.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># <span class="doctag">XXX:</span> Do we need to use a timeout here?</span></span><br><span class="line">        found = <span class="number">0</span></span><br><span class="line">        datas = <span class="variable language_">self</span>.fetch_data(<span class="variable language_">self</span>.redis_key, <span class="variable language_">self</span>.redis_batch_size)</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> datas:</span><br><span class="line">            reqs = <span class="variable language_">self</span>.make_request_from_data(data)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(reqs, Iterable):</span><br><span class="line">                <span class="keyword">for</span> req <span class="keyword">in</span> reqs:</span><br><span class="line">                    <span class="keyword">yield</span> req</span><br><span class="line">                    <span class="comment"># <span class="doctag">XXX:</span> should be here?</span></span><br><span class="line">                    found += <span class="number">1</span></span><br><span class="line">                    <span class="variable language_">self</span>.logger.info(<span class="string">f&quot;start req url:<span class="subst">&#123;req.url&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="keyword">elif</span> reqs:</span><br><span class="line">                <span class="keyword">yield</span> reqs</span><br><span class="line">                found += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="variable language_">self</span>.logger.debug(<span class="string">f&quot;Request not made from data: <span class="subst">&#123;data&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> found:</span><br><span class="line">            <span class="variable language_">self</span>.logger.debug(<span class="string">f&quot;Read <span class="subst">&#123;found&#125;</span> requests from &#x27;<span class="subst">&#123;self.redis_key&#125;</span>&#x27;&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="make-request-from-data（RedisMixin）"><a href="#make-request-from-data（RedisMixin）" class="headerlink" title="make_request_from_data（RedisMixin）"></a>make_request_from_data（RedisMixin）</h3><p>这是一个把redis的字节数据转换成请求的转换函数，是定制化请求的关键。</p>
<p><strong>解析数据</strong>：它接收从Redis取出的原始字节数据<code>data</code>，并尝试将其解码和解析。它优先处理<strong>JSON格式</strong>的字符串。</p>
<p><strong>创建请求</strong>：如果数据是JSON，它可以解析出<code>url</code>、<code>meta</code>（元数据）、<code>method</code>（如’POST’）等多个参数，并创建一个功能更丰富的<code>FormRequest</code>对象。这允许你通过Redis传递非常复杂的请求信息，而不仅仅是一个URL。</p>
<p><strong>兼容旧版</strong>：如果数据只是一个普通的字符串（即一个URL），它会打印一个警告，但仍然会创建一个基本的<code>FormRequest</code>来兼容旧的用法。</p>
<p><strong>错误处理</strong>：如果解析后的数据中没有<code>url</code>字段，它会发出警告并返回空，防止程序出错。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_request_from_data</span>(<span class="params">self, data</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Returns a `Request` instance for data coming from Redis.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Overriding this function to support the `json` requested `data` that contains</span></span><br><span class="line"><span class="string">    `url` ,`meta` and other optional parameters. `meta` is a nested json which contains sub-data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Along with:</span></span><br><span class="line"><span class="string">    After accessing the data, sending the FormRequest with `url`, `meta` and addition `formdata`, `method`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    For example:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. code:: json</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">            &quot;url&quot;: &quot;https://example.com&quot;,</span></span><br><span class="line"><span class="string">            &quot;meta&quot;: &#123;</span></span><br><span class="line"><span class="string">                &quot;job-id&quot;:&quot;123xsd&quot;,</span></span><br><span class="line"><span class="string">                &quot;start-date&quot;:&quot;dd/mm/yy&quot;,</span></span><br><span class="line"><span class="string">            &#125;,</span></span><br><span class="line"><span class="string">            &quot;url_cookie_key&quot;:&quot;fertxsas&quot;,</span></span><br><span class="line"><span class="string">            &quot;method&quot;:&quot;POST&quot;,</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    If `url` is empty, return `[]`. So you should verify the `url` in the data.</span></span><br><span class="line"><span class="string">    If `method` is empty, the request object will set method to &#x27;GET&#x27;, optional.</span></span><br><span class="line"><span class="string">    If `meta` is empty, the request object will set `meta` to an empty dictionary, optional.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This json supported data can be accessed from &#x27;scrapy.spider&#x27; through response.</span></span><br><span class="line"><span class="string">    &#x27;request.url&#x27;, &#x27;request.meta&#x27;, &#x27;request.cookies&#x27;, &#x27;request.method&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    data : bytes</span></span><br><span class="line"><span class="string">        Message from redis.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    formatted_data = bytes_to_str(data, <span class="variable language_">self</span>.redis_encoding)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> is_dict(formatted_data):</span><br><span class="line">        parameter = json.loads(formatted_data)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="variable language_">self</span>.logger.warning(</span><br><span class="line">            <span class="string">f&quot;<span class="subst">&#123;TextColor.WARNING&#125;</span>WARNING: String request is deprecated, please use JSON data format. &quot;</span></span><br><span class="line">            <span class="string">f&quot;Detail information, please check https://github.com/rmax/scrapy-redis#features<span class="subst">&#123;TextColor.ENDC&#125;</span>&quot;</span></span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> FormRequest(formatted_data, dont_filter=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> parameter.get(<span class="string">&quot;url&quot;</span>, <span class="literal">None</span>) <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="variable language_">self</span>.logger.warning(</span><br><span class="line">            <span class="string">f&quot;<span class="subst">&#123;TextColor.WARNING&#125;</span>The data from Redis has no url key in push data<span class="subst">&#123;TextColor.ENDC&#125;</span>&quot;</span></span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> []</span><br><span class="line"></span><br><span class="line">    url = parameter.pop(<span class="string">&quot;url&quot;</span>)</span><br><span class="line">    method = parameter.pop(<span class="string">&quot;method&quot;</span>).upper() <span class="keyword">if</span> <span class="string">&quot;method&quot;</span> <span class="keyword">in</span> parameter <span class="keyword">else</span> <span class="string">&quot;GET&quot;</span></span><br><span class="line">    metadata = parameter.pop(<span class="string">&quot;meta&quot;</span>) <span class="keyword">if</span> <span class="string">&quot;meta&quot;</span> <span class="keyword">in</span> parameter <span class="keyword">else</span> &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> FormRequest(</span><br><span class="line">        url, dont_filter=<span class="literal">True</span>, method=method, formdata=parameter, meta=metadata</span><br><span class="line">    )</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="schedule-next-requests（RedisMixin）"><a href="#schedule-next-requests（RedisMixin）" class="headerlink" title="schedule_next_requests（RedisMixin）"></a>schedule_next_requests（RedisMixin）</h3><p>再次从redis中获得一批request数据，然后将这些请求逐一交给Scrapy引擎（<code>self.crawler.engine.crawl(req)</code>），让引擎去安排下载。</p>
<p>这个函数是给idle函数调用的，当爬虫处于空闲状态，就会调用这个函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">schedule_next_requests</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Schedules a request if available&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> While there is capacity, schedule a batch of redis requests.</span></span><br><span class="line">    <span class="keyword">for</span> req <span class="keyword">in</span> <span class="variable language_">self</span>.next_requests():</span><br><span class="line">        <span class="comment"># see https://github.com/scrapy/scrapy/issues/5994</span></span><br><span class="line">        <span class="keyword">if</span> scrapy_version &gt;= (<span class="number">2</span>, <span class="number">6</span>):</span><br><span class="line">            <span class="variable language_">self</span>.crawler.engine.crawl(req)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.crawler.engine.crawl(req, spider=<span class="variable language_">self</span>)</span><br></pre></td></tr></table></figure>

<h3 id="spider-idle（RedisMixin）"><a href="#spider-idle（RedisMixin）" class="headerlink" title="spider_idle（RedisMixin）"></a>spider_idle（RedisMixin）</h3><p>处理爬虫空闲状态，实现“永不停止”的分布式爬取。</p>
<p><strong>检查Redis</strong>：当Scrapy引擎变为空闲时，此方法被触发。它会首先检查Redis队列中是否还有剩余的URL。</p>
<p><strong>调度新请求</strong>：如果Redis中还有URL，它会调用<code>self.schedule_next_requests()</code>来获取并调度新的请求。</p>
<p><strong>防止关闭</strong>：最关键的一步是，在最后<code>raise DontCloseSpider</code>。这个异常会告诉Scrapy引擎：“请不要关闭我，我可能很快就会有新的任务。” 这样，只要Redis中还有URL，或者有新的URL被不断添加进来，爬虫就会一直运行下去。</p>
<p><strong>超时关闭</strong>：它也会检查空闲时间是否超过了设定的<code>max_idle_time</code>。如果长时间没有从Redis获取到新任务，它就不会抛出<code>DontCloseSpider</code>异常，从而允许爬虫正常关闭，避免了资源的无限期占用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">spider_idle</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Schedules a request if available, otherwise waits.</span></span><br><span class="line"><span class="string">    or close spider when waiting seconds &gt; MAX_IDLE_TIME_BEFORE_CLOSE.</span></span><br><span class="line"><span class="string">    MAX_IDLE_TIME_BEFORE_CLOSE will not affect SCHEDULER_IDLE_BEFORE_CLOSE.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.server <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> <span class="variable language_">self</span>.count_size(<span class="variable language_">self</span>.redis_key) &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="variable language_">self</span>.spider_idle_start_time = <span class="built_in">int</span>(time.time())</span><br><span class="line"></span><br><span class="line">    <span class="variable language_">self</span>.schedule_next_requests()</span><br><span class="line"></span><br><span class="line">    idle_time = <span class="built_in">int</span>(time.time()) - <span class="variable language_">self</span>.spider_idle_start_time</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.max_idle_time != <span class="number">0</span> <span class="keyword">and</span> idle_time &gt;= <span class="variable language_">self</span>.max_idle_time:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="keyword">raise</span> DontCloseSpider</span><br></pre></td></tr></table></figure>

<h3 id="类RedisSpider-RedisCrawlSpider的函数"><a href="#类RedisSpider-RedisCrawlSpider的函数" class="headerlink" title="类RedisSpider&#x2F;RedisCrawlSpider的函数"></a>类RedisSpider&#x2F;RedisCrawlSpider的函数</h3><p>直接继承父类RedisMixin和Spider&#x2F;CrawlSpider。</p>
<p>相当于获得了一个实现了Redis接口的Spider&#x2F;CrawlSpider。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RedisSpider</span>(RedisMixin, Spider):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Spider that reads urls from redis queue when idle.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Attributes</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    redis_key : str (default: REDIS_START_URLS_KEY)</span></span><br><span class="line"><span class="string">        Redis key where to fetch start URLs from..</span></span><br><span class="line"><span class="string">    redis_batch_size : int (default: CONCURRENT_REQUESTS)</span></span><br><span class="line"><span class="string">        Number of messages to fetch from redis on each attempt.</span></span><br><span class="line"><span class="string">    redis_encoding : str (default: REDIS_ENCODING)</span></span><br><span class="line"><span class="string">        Encoding to use when decoding messages from redis queue.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Settings</span></span><br><span class="line"><span class="string">    --------</span></span><br><span class="line"><span class="string">    REDIS_START_URLS_KEY : str (default: &quot;&lt;spider.name&gt;:start_urls&quot;)</span></span><br><span class="line"><span class="string">        Default Redis key where to fetch start URLs from..</span></span><br><span class="line"><span class="string">    REDIS_START_URLS_BATCH_SIZE : int (deprecated by CONCURRENT_REQUESTS)</span></span><br><span class="line"><span class="string">        Default number of messages to fetch from redis on each attempt.</span></span><br><span class="line"><span class="string">    REDIS_START_URLS_AS_SET : bool (default: False)</span></span><br><span class="line"><span class="string">        Use SET operations to retrieve messages from the redis queue. If False,</span></span><br><span class="line"><span class="string">        the messages are retrieve using the LPOP command.</span></span><br><span class="line"><span class="string">    REDIS_ENCODING : str (default: &quot;utf-8&quot;)</span></span><br><span class="line"><span class="string">        Default encoding to use when decoding messages from redis queue.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_crawler</span>(<span class="params">cls, crawler, *args, **kwargs</span>):</span><br><span class="line">        obj = <span class="built_in">super</span>().from_crawler(crawler, *args, **kwargs)</span><br><span class="line">        obj.setup_redis(crawler)</span><br><span class="line">        <span class="keyword">return</span> obj</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RedisCrawlSpider</span>(RedisMixin, CrawlSpider):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Spider that reads urls from redis queue when idle.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Attributes</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    redis_key : str (default: REDIS_START_URLS_KEY)</span></span><br><span class="line"><span class="string">        Redis key where to fetch start URLs from..</span></span><br><span class="line"><span class="string">    redis_batch_size : int (default: CONCURRENT_REQUESTS)</span></span><br><span class="line"><span class="string">        Number of messages to fetch from redis on each attempt.</span></span><br><span class="line"><span class="string">    redis_encoding : str (default: REDIS_ENCODING)</span></span><br><span class="line"><span class="string">        Encoding to use when decoding messages from redis queue.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Settings</span></span><br><span class="line"><span class="string">    --------</span></span><br><span class="line"><span class="string">    REDIS_START_URLS_KEY : str (default: &quot;&lt;spider.name&gt;:start_urls&quot;)</span></span><br><span class="line"><span class="string">        Default Redis key where to fetch start URLs from..</span></span><br><span class="line"><span class="string">    REDIS_START_URLS_BATCH_SIZE : int (deprecated by CONCURRENT_REQUESTS)</span></span><br><span class="line"><span class="string">        Default number of messages to fetch from redis on each attempt.</span></span><br><span class="line"><span class="string">    REDIS_START_URLS_AS_SET : bool (default: True)</span></span><br><span class="line"><span class="string">        Use SET operations to retrieve messages from the redis queue.</span></span><br><span class="line"><span class="string">    REDIS_ENCODING : str (default: &quot;utf-8&quot;)</span></span><br><span class="line"><span class="string">        Default encoding to use when decoding messages from redis queue.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_crawler</span>(<span class="params">cls, crawler, *args, **kwargs</span>):</span><br><span class="line">        obj = <span class="built_in">super</span>().from_crawler(crawler, *args, **kwargs)</span><br><span class="line">        obj.setup_redis(crawler)</span><br><span class="line">        <span class="keyword">return</span> obj</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="管道数据存储（源文件pipelines）"><a href="#管道数据存储（源文件pipelines）" class="headerlink" title="管道数据存储（源文件pipelines）"></a>管道数据存储（源文件pipelines）</h2><p>只提2个地方——process_item和from_crawler。</p>
<h3 id="from-crawler-1"><a href="#from-crawler-1" class="headerlink" title="from_crawler"></a>from_crawler</h3><p>这个函数在很多地方都看到了，这是一个工厂模式的对象创建函数。</p>
<p>from_crawler总会先获得一个类，然后将settings.py或者是其它来源（其它文件或者redis数据库）的配置信息，用于这个类的构造函数，以此创建一个实例。这样子创建出的实例，跟配置文件相关。 </p>
<p>而工厂模式是：将所有组件的对象的实例，统一交给工厂类来完成，而非直接调用它们的构造函数。 </p>
<p>Scrapy引擎在启动后，会通过settings.py获得类RedisPipeline，然后调用它的from_crawler进行实例化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.utils.misc <span class="keyword">import</span> load_object</span><br><span class="line"><span class="keyword">from</span> scrapy.utils.serialize <span class="keyword">import</span> ScrapyJSONEncoder</span><br><span class="line"><span class="keyword">from</span> twisted.internet.threads <span class="keyword">import</span> deferToThread</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> . <span class="keyword">import</span> connection, defaults</span><br><span class="line"></span><br><span class="line">default_serialize = ScrapyJSONEncoder().encode</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RedisPipeline</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Pushes serialized item into a redis list/queue</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Settings</span></span><br><span class="line"><span class="string">    --------</span></span><br><span class="line"><span class="string">    REDIS_ITEMS_KEY : str</span></span><br><span class="line"><span class="string">        Redis key where to store items.</span></span><br><span class="line"><span class="string">    REDIS_ITEMS_SERIALIZER : str</span></span><br><span class="line"><span class="string">        Object path to serializer function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self, server, key=defaults.PIPELINE_KEY, serialize_func=default_serialize</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Initialize pipeline.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        server : StrictRedis</span></span><br><span class="line"><span class="string">            Redis client instance.</span></span><br><span class="line"><span class="string">        key : str</span></span><br><span class="line"><span class="string">            Redis key where to store items.</span></span><br><span class="line"><span class="string">        serialize_func : callable</span></span><br><span class="line"><span class="string">            Items serializer function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.server = server</span><br><span class="line">        <span class="variable language_">self</span>.key = key</span><br><span class="line">        <span class="variable language_">self</span>.serialize = serialize_func</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_settings</span>(<span class="params">cls, settings</span>):</span><br><span class="line">        params = &#123;</span><br><span class="line">            <span class="string">&quot;server&quot;</span>: connection.from_settings(settings),</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> settings.get(<span class="string">&quot;REDIS_ITEMS_KEY&quot;</span>):</span><br><span class="line">            params[<span class="string">&quot;key&quot;</span>] = settings[<span class="string">&quot;REDIS_ITEMS_KEY&quot;</span>]</span><br><span class="line">        <span class="keyword">if</span> settings.get(<span class="string">&quot;REDIS_ITEMS_SERIALIZER&quot;</span>):</span><br><span class="line">            params[<span class="string">&quot;serialize_func&quot;</span>] = load_object(settings[<span class="string">&quot;REDIS_ITEMS_SERIALIZER&quot;</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> cls(**params)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_crawler</span>(<span class="params">cls, crawler</span>):</span><br><span class="line">        <span class="keyword">return</span> cls.from_settings(crawler.settings)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        <span class="keyword">return</span> deferToThread(<span class="variable language_">self</span>._process_item, item, spider)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        key = <span class="variable language_">self</span>.item_key(item, spider)</span><br><span class="line">        data = <span class="variable language_">self</span>.serialize(item)</span><br><span class="line">        <span class="variable language_">self</span>.server.rpush(key, data)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">item_key</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Returns redis key based on given spider.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Override this function to use a different key depending on the item</span></span><br><span class="line"><span class="string">        and/or spider.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.key % &#123;<span class="string">&quot;spider&quot;</span>: spider.name&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="process-item"><a href="#process-item" class="headerlink" title="process_item"></a>process_item</h3><p>Scrapy引擎会将item发送给process_item，而scrapy-redit的process_item只做一件事。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> deferToThread(<span class="variable language_">self</span>._process_item, item, spider)</span><br></pre></td></tr></table></figure>

<p><strong><code>deferToThread</code> 是这里的关键！</strong> 它的意思是“把 <code>_process_item</code> 这个任务扔到另一个<strong>线程</strong>去做”。</p>
<p><strong>为什么这么做？</strong> 因为数据库操作（比如写入Redis）可能会有网络延迟，是<strong>阻塞操作</strong>。Scrapy是基于Twisted构建的<strong>异步</strong>框架，主线程非常宝贵，不能被任何耗时操作卡住。通过将写入操作移到子线程，主线程可以毫无延迟地继续去处理其他网络请求，极大地提高了爬虫的整体效率。这是一种<strong>异步非阻塞</strong>的设计。</p>
<p>在新的线程里，<code>_process_item</code> 方法开始执行实际的存储工作：</p>
<ol>
<li><code>key = self.item_key(item, spider)</code>: 调用 <code>item_key</code> 方法生成最终的Redis键名，默认会把爬虫的名字嵌入进去，例如 <code>my_spider:items</code>。这个方法可以被重写，以实现更复杂的逻辑（比如根据item内容存到不同的key）。</li>
<li><code>data = self.serialize(item)</code>: 调用初始化时设置的序列化函数，将 <code>item</code> 这个Python字典转换成 <code>data</code> (一个JSON字符串)。</li>
<li><code>self.server.rpush(key, data)</code>: 使用Redis的 <code>rpush</code> 命令，将这个JSON字符串推入指定 <code>key</code> 的列表末尾。</li>
<li><code>return item</code>: 将原始的 <code>item</code> 返回。这很重要，因为如果后面还有其他的Pipeline，<code>item</code> 可以继续被传递下去。</li>
</ol>
<h1 id="更多scrapy扩展参考"><a href="#更多scrapy扩展参考" class="headerlink" title="更多scrapy扩展参考"></a>更多scrapy扩展参考</h1><p><a target="_blank" rel="noopener" href="https://docs.scrapy.net.cn/en/latest/topics/extensions.html#module-scrapy.extensions.spiderstate">https://docs.scrapy.net.cn/en/latest/topics/extensions.html#module-scrapy.extensions.spiderstate</a></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB-redis-scrapy-redis-%E7%88%AC%E8%99%AB/" rel="tag"># 分布式爬虫/redis/scrapy-redis/爬虫</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/07/10/%E5%88%A9%E7%94%A8scrapy%E6%8A%93%E5%8F%96%E5%9B%BE%E9%9B%86%EF%BC%88day4%EF%BC%89/" rel="prev" title="利用scrapy抓取图集（day4）">
                  <i class="fa fa-angle-left"></i> 利用scrapy抓取图集（day4）
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">X14Nuy</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">540k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">16:21</span>
  </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>

</body>
</html>
